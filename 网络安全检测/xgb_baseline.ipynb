{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost baseline 87.625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33219, 7), (4000, 7), (37219, 7))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.concat([\n",
    "    pd.read_csv('data/train/SQL注入.csv'),\n",
    "    pd.read_csv('data/train/XSS跨站脚本.csv'),\n",
    "    pd.read_csv('data/train/命令执行.csv'),\n",
    "    pd.read_csv('data/train/白.csv'),\n",
    "    pd.read_csv('data/train/目录遍历.csv'),\n",
    "    pd.read_csv('data/train/远程代码执行.csv'),\n",
    "],axis=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "test=pd.read_csv('data/test.csv')\n",
    "test['lable'] = -1\n",
    "data = pd.concat([train, test])\n",
    "train.shape, test.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37219/37219 [00:05<00:00, 6688.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37219, 16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ua_parser import user_agent_parser\n",
    "def agent_parser(user_agent):\n",
    "    if user_agent != \"Null\":\n",
    "        parse_dict = user_agent_parser.Parse(user_agent)\n",
    "        info_list = {}\n",
    "        user_agent_info = dict(zip(['user_agent_'  + i for i in parse_dict['user_agent'].keys()], parse_dict['user_agent'].values()))\n",
    "        os_info = dict(zip(['os_'  + i for i in parse_dict['os'].keys()], parse_dict['os'].values()))\n",
    "        device_info = dict(zip(['device_'  + i for i in parse_dict['device'].keys()], parse_dict['device'].values()))\n",
    "        temp_dict = {**user_agent_info, **os_info, **device_info}\n",
    "        # df = pd.DataFrame.from_dict(temp_dict, orient='index').T\n",
    "        return temp_dict\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "data['user_agent'] = data['user_agent'].fillna(\"Null\")\n",
    "# temp_df = pd.DataFrame()\n",
    "agent_list = []\n",
    "for i in tqdm(data['user_agent']):\n",
    "    agent_list.append(agent_parser(i))\n",
    "agent_df = pd.DataFrame(agent_list).drop(['os_minor', 'os_patch', 'os_patch_minor'], axis=1).reset_index()\n",
    "data['index'] = agent_df['index'].tolist()\n",
    "data = pd.merge(left=data, right=agent_df, on='index', how='left')\n",
    "data.drop(['index'], axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37219 entries, 0 to 37218\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   id                 37219 non-null  int64 \n",
      " 1   method             37219 non-null  object\n",
      " 2   user_agent         37219 non-null  object\n",
      " 3   url                37219 non-null  object\n",
      " 4   refer              37219 non-null  object\n",
      " 5   body               37219 non-null  object\n",
      " 6   lable              37219 non-null  int64 \n",
      " 7   user_agent_family  37219 non-null  object\n",
      " 8   user_agent_major   37219 non-null  int64 \n",
      " 9   user_agent_minor   37219 non-null  int64 \n",
      " 10  user_agent_patch   37219 non-null  int64 \n",
      " 11  os_family          37219 non-null  object\n",
      " 12  os_major           37219 non-null  object\n",
      " 13  device_family      37219 non-null  object\n",
      " 14  device_brand       37219 non-null  object\n",
      " 15  device_model       37219 non-null  object\n",
      "dtypes: int64(5), object(11)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "num_ua = ['user_agent_major', 'user_agent_minor', 'user_agent_patch']\n",
    "for i in data.columns[:]:\n",
    "    if i in num_ua:\n",
    "        data[i] = data[i].fillna('None')\n",
    "        data[i] = data[i].replace({\"None\": '-999', 'b13pre': '13'})\n",
    "        data[i] = data[i].astype('int')\n",
    "        # print(data[i].unique())\n",
    "    else:\n",
    "        data[i] = data[i].fillna(\"None\")\n",
    "    \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37219, 48), (37219, 64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# def tfidf_embeding(cols_name, n_components=16, min_df=3, max_df=0.5, ngram_range=(2, 6)):\n",
    "#     texts=data[cols_name].str[:512].values.tolist()\n",
    "#     tf = TfidfVectorizer(min_df= min_df, max_df=max_df, analyzer = 'char_wb', ngram_range=ngram_range)\n",
    "#     X = tf.fit_transform(texts)\n",
    "#     svd = TruncatedSVD(n_components=n_components,\n",
    "#                     random_state=42)\n",
    "#     X_svd = svd.fit_transform(X)\n",
    "#     df_tfidf = pd.DataFrame(X_svd)\n",
    "#     df_tfidf.columns = [cols_name + f'_tfidf_{i}' for i in range(n_components)]\n",
    "#     return df_tfidf\n",
    "\n",
    "# df_tfidf = pd.DataFrame()\n",
    "# texts_col = ['user_agent', 'url', 'body']\n",
    "\n",
    "# user_agent_tfidf = tfidf_embeding('user_agent')\n",
    "# for i in tqdm(texts_col):\n",
    "#     df_tfidf = pd.concat([df_tfidf, tfidf_embeding(i)], axis=1)\n",
    "# df_tfidf.to_csv(\"df_tfidf.csv\", index=False)\n",
    "df_tfidf = pd.read_csv(\"df_tfidf.csv\")\n",
    "data = pd.concat([data, df_tfidf], axis=1)\n",
    "df_tfidf.shape, data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['method', 'user_agent', 'refer', 'user_agent_family', 'os_family', 'os_major', 'device_family', 'device_brand', 'device_model']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37219, 73)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "cat_cols = []\n",
    "for i in data.columns:\n",
    "    if data[i].dtype == 'object' and i not in ['url', 'body']:\n",
    "    # if data[i].dtype == 'object':\n",
    "        cat_cols.append(i)\n",
    "print(cat_cols)\n",
    "lbe = LabelEncoder()\n",
    "\n",
    "for i in cat_cols:\n",
    "    data[i] = lbe.fit_transform(data[i])\n",
    "    data[i + '_couts'] = data.groupby(i)[i].transform('count')\n",
    "train, test = data[:len(train)], data[len(train):]\n",
    "data.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "# #需要编码的特征\n",
    "# # enc_list=['trademark_cn','brand_cn','make_cn','series','client_no','p1_census_register']\n",
    "# enc_list = cat_cols\n",
    "\n",
    "# #标签\n",
    "# label='lable'\n",
    "# for f in enc_list:\n",
    "#     train[f + '_target_enc'] = 0\n",
    "#     test[f + '_target_enc'] = 0\n",
    "#     for i, (trn_idx, val_idx) in enumerate(skf.split(train,train['lable'])):\n",
    "#         trn_x = train[[f, label]].iloc[trn_idx].reset_index(drop=True)\n",
    "#         val_x = train[[f]].iloc[val_idx].reset_index(drop=True)\n",
    "#         enc_df = trn_x.groupby(f, as_index=False)[label].agg({f + '_target_enc': 'mean'})\n",
    "#         val_x = val_x.merge(enc_df, on=f, how='left')\n",
    "#         test_x = test[[f]].merge(enc_df, on=f, how='left')\n",
    "#         val_x[f + '_target_enc'] = val_x[f + '_target_enc'].fillna(train[label].mean())\n",
    "#         test_x[f + '_target_enc'] = test_x[f + '_target_enc'].fillna(train[label].mean())\n",
    "#         train.loc[val_idx, f + '_target_enc'] = val_x[f + '_target_enc'].values\n",
    "#         test[f + '_target_enc'] += test_x[f + '_target_enc'].values / skf.n_splits\n",
    "\n",
    "\n",
    "# train=train.drop(labels=cat_cols, axis=1)\n",
    "# test=test.drop(labels=cat_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "def xgb_model(train, target, test, k):\n",
    "    feats = [f for f in train.columns if f not in ['lable',  'url', 'body', 'id']]\n",
    "    print(\"Current num of features:\", len(feats))\n",
    "    oof_probs = np.zeros((train.shape[0], 6))\n",
    "    output_preds = 0\n",
    "    offline_score = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    params = {\n",
    "        'eta': 0.05,\n",
    "        'max_depth': 8,\n",
    "        'booster': 'gbtree',\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 6,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'nthread': 4,\n",
    "        'subsample': 0.7,\n",
    "        # 'min_child_weight': 3,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'seed':623,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'gpu_id': 0\n",
    "    }\n",
    "    seeds = [623]\n",
    "    for seed in seeds:\n",
    "        folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "            print(f\"\\n------------Fold{i}----------------\\n\")\n",
    "            train_y, test_y = target.iloc[train_index], target.iloc[test_index]\n",
    "            train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "            dtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "            dval = xgb.DMatrix(test_X, label=test_y)\n",
    "            xgb_model = xgb.train(params, \n",
    "                                  dtrain, \n",
    "                                  num_boost_round=10000,\n",
    "                                  evals = [(dval, 'eval')],\n",
    "                                  early_stopping_rounds=100,\n",
    "                                  verbose_eval=100\n",
    "                                  )\n",
    "\n",
    "            oof_probs[test_index] = xgb_model.predict(xgb.DMatrix(test_X[feats]),\n",
    "                                                       iteration_range=(xgb_model.best_iteration-1,  xgb_model.best_iteration+1)) / len(\n",
    "                seeds)\n",
    "            # offline_score.append(xgb.best_score['valid_0']['mlogloss'])\n",
    "            output_preds += xgb_model.predict(xgb.DMatrix(test[feats]),\n",
    "                                        iteration_range=(xgb_model.best_iteration-1,  xgb_model.best_iteration+1)) / folds.n_splits / len(seeds)\n",
    "            # feature importance\n",
    "            # fold_importance_df = pd.DataFrame()\n",
    "            # fold_importance_df[\"feature\"] = feats\n",
    "            # fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
    "            # fold_importance_df[\"fold\"] = i + 1\n",
    "            # feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "            \n",
    "    # print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "    # print('feature importance:')\n",
    "    # print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "\n",
    "    return output_preds, oof_probs\n",
    "\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "开始XGB模型训练train\n",
      "Current num of features: 69\n",
      "\n",
      "------------Fold0----------------\n",
      "\n",
      "[0]\teval-mlogloss:1.66166\n",
      "[100]\teval-mlogloss:0.08609\n",
      "[200]\teval-mlogloss:0.06773\n",
      "[299]\teval-mlogloss:0.06958\n",
      "\n",
      "------------Fold1----------------\n",
      "\n",
      "[0]\teval-mlogloss:1.66268\n",
      "[100]\teval-mlogloss:0.08999\n",
      "[200]\teval-mlogloss:0.07295\n",
      "[300]\teval-mlogloss:0.07425\n",
      "[330]\teval-mlogloss:0.07500\n",
      "\n",
      "------------Fold2----------------\n",
      "\n",
      "[0]\teval-mlogloss:1.66269\n",
      "[100]\teval-mlogloss:0.08732\n",
      "[200]\teval-mlogloss:0.06743\n",
      "[300]\teval-mlogloss:0.06753\n",
      "[341]\teval-mlogloss:0.06816\n",
      "\n",
      "------------Fold3----------------\n",
      "\n",
      "[0]\teval-mlogloss:1.66190\n",
      "[100]\teval-mlogloss:0.08037\n",
      "[200]\teval-mlogloss:0.06031\n",
      "[300]\teval-mlogloss:0.05997\n",
      "[361]\teval-mlogloss:0.06074\n",
      "\n",
      "------------Fold4----------------\n",
      "\n",
      "[0]\teval-mlogloss:1.66047\n",
      "[100]\teval-mlogloss:0.08529\n",
      "[200]\teval-mlogloss:0.06543\n",
      "[300]\teval-mlogloss:0.06568\n",
      "[325]\teval-mlogloss:0.06620\n"
     ]
    }
   ],
   "source": [
    "feature_names = list(filter(\n",
    "    lambda x: x not in ['lable', 'id', 'url', 'body'], train.columns\n",
    "    \n",
    "))\n",
    "print(len(feature_names))\n",
    "\n",
    "print('开始XGB模型训练train')\n",
    "xgb_preds, xgb_oof = xgb_model(train=train[feature_names],\n",
    "                               target=train['lable'],\n",
    "                               test=test[feature_names], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6917426773834251\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(train['lable'],np.argmax(xgb_oof,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv('data/submit_example.csv')\n",
    "sub['predict']=np.argmax(xgb_preds,axis=1)\n",
    "sub.to_csv('xgb_sub.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8e0e84b0ce89630bce80742e7f2333d8dc576d25f126d2a52c4899e8862e05a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
