{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33219, 7), (4000, 7), (37219, 7))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.concat([\n",
    "    pd.read_csv('data/train/SQL注入.csv'),\n",
    "    pd.read_csv('data/train/XSS跨站脚本.csv'),\n",
    "    pd.read_csv('data/train/命令执行.csv'),\n",
    "    pd.read_csv('data/train/白.csv'),\n",
    "    pd.read_csv('data/train/目录遍历.csv'),\n",
    "    pd.read_csv('data/train/远程代码执行.csv'),\n",
    "],axis=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "test=pd.read_csv('data/test.csv')\n",
    "test['lable'] = -1\n",
    "data = pd.concat([train, test])\n",
    "train.shape, test.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37219/37219 [00:03<00:00, 11030.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37219, 16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cmath import nan\n",
    "from ua_parser import user_agent_parser\n",
    "def agent_parser(user_agent):\n",
    "    if user_agent != \"Null\":\n",
    "        parse_dict = user_agent_parser.Parse(user_agent)\n",
    "        info_list = {}\n",
    "        user_agent_info = dict(zip(['user_agent_'  + i for i in parse_dict['user_agent'].keys()], parse_dict['user_agent'].values()))\n",
    "        os_info = dict(zip(['os_'  + i for i in parse_dict['os'].keys()], parse_dict['os'].values()))\n",
    "        device_info = dict(zip(['device_'  + i for i in parse_dict['device'].keys()], parse_dict['device'].values()))\n",
    "        temp_dict = {**user_agent_info, **os_info, **device_info}\n",
    "        # df = pd.DataFrame.from_dict(temp_dict, orient='index').T\n",
    "        return temp_dict\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "data['user_agent'] = data['user_agent'].fillna(\"Null\")\n",
    "# temp_df = pd.DataFrame()\n",
    "agent_list = []\n",
    "for i in tqdm(data['user_agent']):\n",
    "    agent_list.append(agent_parser(i))\n",
    "agent_df = pd.DataFrame(agent_list).drop(['os_minor', 'os_patch', 'os_patch_minor'], axis=1).reset_index()\n",
    "data['index'] = agent_df['index'].tolist()\n",
    "data = pd.merge(left=data, right=agent_df, on='index', how='left')\n",
    "data.drop(['index'], axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37219 entries, 0 to 37218\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   id                 37219 non-null  int64 \n",
      " 1   method             37219 non-null  object\n",
      " 2   user_agent         37219 non-null  object\n",
      " 3   url                37219 non-null  object\n",
      " 4   refer              37219 non-null  object\n",
      " 5   body               37219 non-null  object\n",
      " 6   lable              37219 non-null  int64 \n",
      " 7   user_agent_family  37219 non-null  object\n",
      " 8   user_agent_major   37219 non-null  int64 \n",
      " 9   user_agent_minor   37219 non-null  int64 \n",
      " 10  user_agent_patch   37219 non-null  int64 \n",
      " 11  os_family          37219 non-null  object\n",
      " 12  os_major           37219 non-null  object\n",
      " 13  device_family      37219 non-null  object\n",
      " 14  device_brand       37219 non-null  object\n",
      " 15  device_model       37219 non-null  object\n",
      "dtypes: int64(5), object(11)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "num_ua = ['user_agent_major', 'user_agent_minor', 'user_agent_patch']\n",
    "for i in data.columns[:]:\n",
    "    if i in num_ua:\n",
    "        data[i] = data[i].fillna('None')\n",
    "        data[i] = data[i].replace({\"None\": '-999', 'b13pre': '13'})\n",
    "        data[i] = data[i].astype('int')\n",
    "        # print(data[i].unique())\n",
    "    else:\n",
    "        data[i] = data[i].fillna(\"None\")\n",
    "    \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:48<00:00, 36.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((37219, 48), (37219, 64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_embeding(cols_name, n_components=16, min_df=3, max_df=0.5, ngram_range=(2, 6)):\n",
    "    texts=data[cols_name].str[:512].values.tolist()\n",
    "    tf = TfidfVectorizer(min_df= min_df, max_df=max_df, analyzer = 'char_wb', ngram_range=ngram_range)\n",
    "    X = tf.fit_transform(texts)\n",
    "    svd = TruncatedSVD(n_components=n_components,\n",
    "                    random_state=42)\n",
    "    X_svd = svd.fit_transform(X)\n",
    "    df_tfidf = pd.DataFrame(X_svd)\n",
    "    df_tfidf.columns = [cols_name + f'_tfidf_{i}' for i in range(n_components)]\n",
    "    return df_tfidf\n",
    "\n",
    "df_tfidf = pd.DataFrame()\n",
    "texts_col = ['user_agent', 'url', 'body']\n",
    "\n",
    "user_agent_tfidf = tfidf_embeding('user_agent')\n",
    "for i in tqdm(texts_col):\n",
    "    df_tfidf = pd.concat([df_tfidf, tfidf_embeding(i)], axis=1)\n",
    "data = pd.concat([data, df_tfidf], axis=1)\n",
    "df_tfidf.to_csv(\"df_tfidf.csv\", index=False)\n",
    "df_tfidf.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['method', 'user_agent', 'refer', 'user_agent_family', 'os_family', 'os_major', 'device_family', 'device_brand', 'device_model']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37219, 73)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "cat_cols = []\n",
    "for i in data.columns:\n",
    "    if data[i].dtype == 'object' and i not in ['url', 'body']:\n",
    "    # if data[i].dtype == 'object':\n",
    "        cat_cols.append(i)\n",
    "print(cat_cols)\n",
    "lbe = LabelEncoder()\n",
    "\n",
    "for i in cat_cols:\n",
    "    data[i] = lbe.fit_transform(data[i])\n",
    "    data[i + '_couts'] = data.groupby(i)[i].transform('count')\n",
    "train, test = data[:len(train)], data[len(train):]\n",
    "data.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "# #需要编码的特征\n",
    "# # enc_list=['trademark_cn','brand_cn','make_cn','series','client_no','p1_census_register']\n",
    "# enc_list = cat_cols\n",
    "\n",
    "# #标签\n",
    "# label='lable'\n",
    "# for f in enc_list:\n",
    "#     train[f + '_target_enc'] = 0\n",
    "#     test[f + '_target_enc'] = 0\n",
    "#     for i, (trn_idx, val_idx) in enumerate(skf.split(train,train['lable'])):\n",
    "#         trn_x = train[[f, label]].iloc[trn_idx].reset_index(drop=True)\n",
    "#         val_x = train[[f]].iloc[val_idx].reset_index(drop=True)\n",
    "#         enc_df = trn_x.groupby(f, as_index=False)[label].agg({f + '_target_enc': 'mean'})\n",
    "#         val_x = val_x.merge(enc_df, on=f, how='left')\n",
    "#         test_x = test[[f]].merge(enc_df, on=f, how='left')\n",
    "#         val_x[f + '_target_enc'] = val_x[f + '_target_enc'].fillna(train[label].mean())\n",
    "#         test_x[f + '_target_enc'] = test_x[f + '_target_enc'].fillna(train[label].mean())\n",
    "#         train.loc[val_idx, f + '_target_enc'] = val_x[f + '_target_enc'].values\n",
    "#         test[f + '_target_enc'] += test_x[f + '_target_enc'].values / skf.n_splits\n",
    "\n",
    "\n",
    "# train=train.drop(labels=cat_cols, axis=1)\n",
    "# test=test.drop(labels=cat_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "\n",
    "def lgb_model(train, target, test, k):\n",
    "    feats = [f for f in train.columns if f not in ['lable',  'url', 'body', 'id']]\n",
    "    # feats = [f for f in train.columns if f not in ['lable', 'id']]\n",
    "    #     feats=import_cols\n",
    "    print('Current num of features:', len(feats))\n",
    "\n",
    "    oof_probs = np.zeros((train.shape[0],6))\n",
    "    output_preds = 0\n",
    "    offline_score = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    parameters = {\n",
    "        'learning_rate': 0.04,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'multiclass',\n",
    "        'metric': 'multi_error',\n",
    "        'num_class': 6,\n",
    "        'num_leaves': 2**5 - 1,\n",
    "        'feature_fraction': 0.6,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'min_data_in_leaf': 15,\n",
    "        'verbose': -1,\n",
    "        'nthread': 4,\n",
    "        'max_depth': 8\n",
    "    }\n",
    "\n",
    "    seeds = [623]\n",
    "    for seed in seeds:\n",
    "        folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "            print(f\"\\n------------Fold{i}----------------\\n\")\n",
    "            train_y, test_y = target.iloc[train_index], target.iloc[test_index]\n",
    "            train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "\n",
    "            dtrain = lgb.Dataset(train_X,\n",
    "                                 label=train_y)\n",
    "            dval = lgb.Dataset(test_X,\n",
    "                               label=test_y)\n",
    "            lgb_model = lgb.train(\n",
    "                parameters,\n",
    "                dtrain,\n",
    "                num_boost_round=8000,\n",
    "                valid_sets=[dval],\n",
    "                callbacks=[early_stopping(100), log_evaluation(100)],\n",
    "            )\n",
    "            oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration) / len(\n",
    "                seeds)\n",
    "            offline_score.append(lgb_model.best_score['valid_0']['multi_error'])\n",
    "            output_preds += lgb_model.predict(test[feats],\n",
    "                                              num_iteration=lgb_model.best_iteration) / folds.n_splits / len(seeds)\n",
    "            print(offline_score)\n",
    "            # feature importance\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
    "            fold_importance_df[\"fold\"] = i + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "    print('feature importance:')\n",
    "    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "\n",
    "    return output_preds, oof_probs, np.mean(offline_score), feature_importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "开始lgb模型训练train\n",
      "Current num of features: 69\n",
      "\n",
      "------------Fold0----------------\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's multi_error: 0.0234798\n",
      "[200]\tvalid_0's multi_error: 0.0224262\n",
      "[300]\tvalid_0's multi_error: 0.0218242\n",
      "Early stopping, best iteration is:\n",
      "[231]\tvalid_0's multi_error: 0.0215232\n",
      "[0.02152317880794702]\n",
      "\n",
      "------------Fold1----------------\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's multi_error: 0.0243829\n",
      "[200]\tvalid_0's multi_error: 0.0234798\n",
      "[300]\tvalid_0's multi_error: 0.0225768\n",
      "Early stopping, best iteration is:\n",
      "[290]\tvalid_0's multi_error: 0.0221252\n",
      "[0.02152317880794702, 0.022125225767609873]\n",
      "\n",
      "------------Fold2----------------\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's multi_error: 0.0242324\n",
      "[200]\tvalid_0's multi_error: 0.0219747\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's multi_error: 0.0218242\n",
      "[0.02152317880794702, 0.022125225767609873, 0.021824202287778448]\n",
      "\n",
      "------------Fold3----------------\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's multi_error: 0.0189645\n",
      "[200]\tvalid_0's multi_error: 0.0176099\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid_0's multi_error: 0.0176099\n",
      "[0.02152317880794702, 0.022125225767609873, 0.021824202287778448, 0.01760987357013847]\n",
      "\n",
      "------------Fold4----------------\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's multi_error: 0.0225802\n",
      "[200]\tvalid_0's multi_error: 0.021978\n",
      "[300]\tvalid_0's multi_error: 0.0215264\n",
      "[400]\tvalid_0's multi_error: 0.0213759\n",
      "Early stopping, best iteration is:\n",
      "[363]\tvalid_0's multi_error: 0.0209243\n",
      "[0.02152317880794702, 0.022125225767609873, 0.021824202287778448, 0.01760987357013847, 0.0209242811982538]\n",
      "OOF-MEAN-AUC:0.020801, OOF-STD-AUC:0.001644\n",
      "feature importance:\n",
      "feature\n",
      "url_tfidf_0                92952.569243\n",
      "user_agent_family          72210.479580\n",
      "url_tfidf_3                54962.755514\n",
      "body_tfidf_0               53277.050896\n",
      "url_tfidf_1                52173.516568\n",
      "user_agent_family_couts    36422.465723\n",
      "url_tfidf_9                35730.131982\n",
      "url_tfidf_10               34160.272673\n",
      "url_tfidf_13               31551.923597\n",
      "url_tfidf_5                26040.236318\n",
      "body_tfidf_1               24744.043656\n",
      "user_agent_tfidf_2         23448.826201\n",
      "body_tfidf_3               23148.095042\n",
      "body_tfidf_5               14198.807229\n",
      "body_tfidf_11              13728.068074\n",
      "url_tfidf_2                12545.386471\n",
      "url_tfidf_15               12442.837871\n",
      "url_tfidf_4                12365.315809\n",
      "url_tfidf_11               12280.737200\n",
      "method                     12159.392844\n",
      "url_tfidf_12               12011.690563\n",
      "user_agent_tfidf_6         10963.216132\n",
      "body_tfidf_12              10060.426124\n",
      "user_agent_couts            9634.106849\n",
      "url_tfidf_8                 9326.515444\n",
      "os_major                    9158.006666\n",
      "user_agent_tfidf_4          9085.674049\n",
      "body_tfidf_4                9057.420410\n",
      "body_tfidf_7                8589.675704\n",
      "user_agent_tfidf_1          8549.497357\n",
      "body_tfidf_14               8058.232650\n",
      "url_tfidf_14                8040.851392\n",
      "user_agent                  7748.733322\n",
      "user_agent_tfidf_13         7667.862765\n",
      "refer_couts                 7651.386755\n",
      "body_tfidf_13               7420.839540\n",
      "body_tfidf_15               6654.485931\n",
      "body_tfidf_8                6123.779968\n",
      "user_agent_tfidf_14         6087.579327\n",
      "user_agent_tfidf_7          6046.601785\n",
      "url_tfidf_7                 6032.737653\n",
      "user_agent_tfidf_5          6020.437802\n",
      "body_tfidf_10               5754.460646\n",
      "body_tfidf_2                5733.514119\n",
      "user_agent_major            5565.407420\n",
      "url_tfidf_6                 5471.894514\n",
      "refer                       5281.535158\n",
      "user_agent_tfidf_8          5056.097303\n",
      "user_agent_tfidf_0          4755.023525\n",
      "user_agent_tfidf_15         3718.778205\n",
      "Name: importance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "feature_names = list(filter(\n",
    "    lambda x: x not in ['lable', 'id', 'url', 'body'], train.columns\n",
    "    \n",
    "))\n",
    "print(len(feature_names))\n",
    "\n",
    "print('开始lgb模型训练train')\n",
    "lgb_preds, lgb_oof, lgb_score, feature_importance_df = lgb_model(train=train[feature_names],\n",
    "                                                                 target=train['lable'],\n",
    "                                                                 test=test[feature_names], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9791986513742136\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(train['lable'],np.argmax(lgb_oof,axis=1)))\n",
    "# sub=pd.read_csv('data/submit_example.csv')\n",
    "# sub['predict']=np.argmax(lgb_preds,axis=1)\n",
    "# sub.to_csv('new_sub.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8e0e84b0ce89630bce80742e7f2333d8dc576d25f126d2a52c4899e8862e05a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
